---
title: "Projet Covid"
author : "Margaux Bailleul / Oriane Duclos / Marie Guibert" 
output: pdf_document
date: "`r Sys.Date()`"
---

```{r message = FALSE}
library(tidyverse)
library(forecast)
library(tidyquant)
library(caschrono)
library(stats)
library(tseries)
library(lmtest)
```

# Importations des données 

Tout d'abord, on importe les données et on sélectionne les données concernant la France. 

```{r eval=FALSE, include=FALSE}
donnees <- read.csv("owid-covid-data.csv",sep=",",stringsAsFactors = T)
donnees_modif <- donnees |> 
  filter(iso_code == "FRA") |> 
  select(date,new_cases)
summary(donnees_modif)

write.table(x = donnees_modif, file = "covid_france.csv", sep = ",")
```


```{r}
donnees_fr <- read.csv("covid_france.csv",sep=",")
summary(donnees_fr)
```

Les données ci-dessus comprennent une variable temporelle et une variable caractérisée par un enregistrement journalier des nouveaux cas de Covid-19 en France. 

```{r}
min(donnees_fr$date)
max(donnees_fr$date)
```
Grâce à cette étape, nous pouvons observer que notre série temporelle débute le 1er Mars 2020 et se termine le 19 Avril 2023. Notre étude a donc une plage d'environ de 3 ans.

## Transformation des données en série temporelle

Premièrement, nous allons transformer nos données en séries temporelles pour pouvoir réaliser notre analyse. 

```{r}
ts_donnees_fr <- ts(donnees_fr$new_cases,start = c(2020,1,3), frequency = 365)
class(ts_donnees_fr)
```

# Prise en main du jeu de données

```{r}
plot(ts_donnees_fr)
```

Ce premier graphique nous montre une hausse brutale des nouveaux cas de covids en 2022. Afin de pouvoir continuer notre analyse de façon cohérente, nous allons diviser notre série en 3 parties : avant, pendant et après ce choc en 2022.  



Nous allons appliquer la décomposition saisonnière à la série temporelle pour visualiser les tendances et les motifs saisonniers.Nous supprimons les données manquantes afin de ne garder que celles qui sont pertinentes.

```{r}
decomp_ts <- stl(na.omit(ts_donnees_fr), s.window = "periodic")
autoplot(decomp_ts)
```

Cette étape nous permet d'observer une tendance à la hausse entre 2020 et 2022, puis à partir de 2022, une tendance à la baisse. \
De plus, nous pouvons voir une saisonnalité annuelle.


## Division de notre série 

Nous décidons de créer trois sous-séries de notre série initiale afin de pouvoir réaliser le traitement des données. Notre objectif est d'isoler le cas particulier de l'année 2022 pour avoir une étude correcte.

```{r}
serie1 <- donnees_fr |> 
  filter(date<="2021-12-22")
# serie1
ts_serie1 <- ts(serie1$new_cases,start = c(2020,1,3), frequency = 365)



serie2 <- donnees_fr |> 
  filter(date>"2021-12-22", date<="2023-01-05")
# serie1
ts_serie2 <- ts(serie2$new_cases,start = c(2021,31,12), frequency = 365)



serie3 <- donnees_fr |> 
  filter(date>"2023-01-05")
# serie3
ts_serie3 <- ts(serie3$new_cases,start = c(2023,2,5), frequency = 365)
```

Nous avons choisi de scinder notre série en trois périodes : 

- avant le 22 Décembre 2021

- entre le 23 Décembre 2021 et le 5 Janvier 2023

- après le 6 Janvier 2023


Nous pouvons maintenant les visualiser : 

```{r}
plot(ts_serie1,main="Nouveaux cas de covid-19 en France entre 2020 et 2022")
```
Dans cette sous-série, nous pouvons observer une saisonnalité avec des pics lors de la fin de l'année, pouvant correspondre à la période hivernale mais aussi au niveau des vacances scolaires (vers le mois de Mars). Ce constat est expliqué par les mouvements de foule et le déplacement des populations. 

```{r}
plot(ts_serie2,main="Nouveaux cas de covid-19 en France entre 2022 et 2023")
```
Cette seconde série chronologique nous montre une tendance à la baisse des nouveaux cas de covid 19. Aussi, nous remarquons une saisonnalité environ tous les deux mois. Au début de l'année 2021 le nombre de nouveaux cas est nettement plus important qu'en 2022.

```{r}
plot(ts_serie3,main="Nouveaux cas de covid-19 en France en 2023")
```
Enfin, cette sous-série est caractérisée par une tendance à la baisse dans un premier temps puis à la hausse. Ce constat est peut-être expliqué par la reprise de la vie active de la population française.

Grâce à cette division, nous allons pouvoir sélectionner la sous-série la plus pertinente.

# Analyse de la première sous-série

Nous avons décidé de nous focaliser sur la première sous-série. \
Ce choix est expliqué grâce à notre connaissance des évènements durant cette année particulière. En effet, les confinements ont pu avoir des conséquences sur notre série et nos données.
Notre étude commence donc le 1er Mars 2020 et s'étend jusqu'au 22 décembre 2021.

Pour rappel, notre série présente une tendance à la hausse comme le montre le graphique ci-dessous. Elle présente aussi une saisonnalité, mais elle n'est pas régulière. En effet, les différentes hausses de nouveaux cas de covid dépendent des confinements et des mesures sanitaires mises en place.

```{r}
autoplot(ts_serie1)+
  geom_smooth(method = lm,color="blue")+
  ggtitle("Nouveaux cas de covids en France entre 2020 et 2022")
```

On va d'abord chercher à décrire notre série grâce à des indicateurs descriptifs simples.

```{r}
mean(ts_serie1)
```

Entre le 1er Mars 2020 et le 22 Décembre 2022, la moyenne des nouveaux cas de covids par jour était de 11 763 cas en France.

```{r}
ts_serie1 |> 
  ggtsdisplay(plot.type = "scatter",smooth=FALSE)
```

Nous pouvons faire quelques observations sur le graphique de l'ACF. Ce graphique nous permet de détecter une structure de corrélation du réseau. Dans notre cas, plusieurs autocorrélations présentent des valeurs significativement non nulles, ce qui signifie que la série chronologique n'est pas aléatoire. \
Aussi, nous pouvons observer un nuage de points plutôt aligné, on peut donc se poser la question d'une éventuelle corrélation.


Afin d'avoir une analyse plus exhaustive, nous pouvons analyser l'ACF et la PACF. En effet, l'étude de l'ACF va nous permettre de détecter la périodicité de la série. 

```{r}
acf <- acf(ts_serie1)
print(data.frame(acf$lag,acf$acf))
```

Ce graphique nous permet d'observer une corrélation hebdomadaire. En effet, nous pouvons remarquer un pic plus élevé tous les 7 jours. 

Puisque notre série montre une corrélation hebdomadaire, nous avons choisi d'étudier une série temporelle avec une fréquence de 7 jours.

```{r}
ts_serie_semaine <- ts(ts_serie1, frequency = 7)
autoplot(ts_serie_semaine) # Visualisation des données
```

```{r}
decomp_ts <- stl(na.omit(ts_serie_semaine), s.window = "periodic")
autoplot(decomp_ts)
```

## Retrait de la tendance / saisonnalité

Nous cherchons à nous ramener à une série sans tendance. Afin de la retirer de notre série, nous allons utiliser l'opérateur diff. Nous allons donc construire un filtre permettant de construire notre analyse. \

Tout d'abord, nous avons décider de ne pas transformer notre série en logarithme. Celle-ci nous permettrait de réduire sa variance mais puisque la série présente des valeurs nulles, cette transformation n'est pas pertinente. \

Nous allons donc différencier la série.

```{r}
ndiffs(ts_serie_semaine) # On nous conseille de différencier la série 1 fois

serie_lissee <- ts_serie_semaine |> 
  diff(lag=1)

ndiffs(serie_lissee) # la série a été différencié comme il faut
```
Puisque l'on différencie une seule fois, la variance augmente mais reste plus faible que si l'on avait différencié 3 ou 4 fois par exemple.

```{r}
serie_lissee |> 
  ggtsdisplay(plot.type = "scatter",smooth=FALSE)
```

Grâce à cette méthode, nous pouvons faire plusieurs constats :

- **Le chronogramme** : 
Notre différenciation nous permet de stationariser notre série et supprimer la tendance. Nous pouvons donc émettre l'hypothèse d'un modèle polynomial. Cela nous permet aussi d'écarter l'hypothèse d'un modèle linéaire.

- **L'ACF** :
L'auto-corrélation décroit car on fait des moyennes sur de moins en moins de valeurs. 
De plus, nous pouvons observer une saisonnalité hebdomadaire avec un pic tous les 7 jours (lag).

- **Corrélation** :
Le nuage de points ne présente plus de direction, il n'existe donc plus de réelle corrélation.

## Stationnarité

Ensuite, nous allons tester la stationnarité de notre série :

```{r}
kpss.test(serie_lissee)
```
Nous avons une p-value associée au test supérieure à 5%, nous rejettons donc l'hypothèse nulle de stationnarité. Notre série n'est donc pas stationnaire.


```{r}
adf.test(serie_lissee)
```
Lors de ce test, nous obtenons une p-value inférieure à 5%. Nous acceptons l'hypothèse de non-stationnarité. Ce test nous permet de confirmer notre hypothèse précédente.

En conclusion, notre série n'est pas **stationnaire**. Cette conclusion nous permet de confirmer que la mise en place d'un modèle linéaire n'est pas une solution pour modéliser notre série. Notre série n'étant pas stationnaire, nos différents seront moins fiables et nos prévisions moins précises. 

# Modélisation de notre série

Nous allons essayer de choisir le meilleur modèle afin d'estimer notre série.

Afin de pouvoir l'estimer, nous avons utiliser la fonction auto.arima() du package forecast qui permet d'effectuer une modélisation automatique. En précisant les arguments trace=T et ic=aic, nous avons donné la main au logiciel R de selectionner le meilleur modèle sur la base du critère AIC. 

## Modèle ARIMA 

```{r}
model_arima <- auto.arima(ts_serie_semaine, ic = "aic",trace=TRUE)
model_arima
```

Modèles identifiés : ARIMA(1,0,1)(0,1,1)

```{r}
summary(model_arima)
```

```{r}
t_stat(model_arima)
```
Le modèle n'est pas simplifiable.


### Modèle polynomial 

Avant de pouvoir faire un modèle polynomial, il faut vérifier la normalité des résidus. Nous pouvons effectuer ceci grâce à un test de Shapiro.


```{r}
shapiro.test(ts_serie_semaine)
```

La p-value est inférieure à 5%, ce qui nous amène à rejeter l'hypothèse nulle. Nos résidus suivent donc une loi normale. 

Nous pouvons alors construire notre modèle polynomial. 

Il peut être utile de modéliser le nombre de nouveaux cas de COVID-19 en utilisant une méthode de régression polynomiale. Les données montrent une tendance générale à la hausse au fil du temps, une régression polynomiale peut être utilisée pour décrire cette tendance. 

```{r}
plot(diff(ts_serie_semaine, differences = 1),type="l")
plot(diff(ts_serie_semaine, differences = 2),type="l")
plot(diff(ts_serie_semaine, differences = 3),type="l")
plot(diff(ts_serie_semaine, differences = 4),type="l")
plot(diff(ts_serie_semaine, differences = 5),type="l")
```
Le degré d de la tendance polynomiale est 3. On a un graphique avec d = 3qui est à peu près centré donc on choisit d-1 = 2.\
La période est T = 7 car nous avons des données hebdomadaires. \

Nous avons donc créé un modèle polynomial de degré 2 pour modéliser la série. 
```{r}
model <- lm(ts_serie_semaine ~ poly(ts_serie_semaine, 2, raw=TRUE))
summary(model)
```


Nous devons donc avoir 3 régresseurs pour la tendance et 6 régresseurs pour la saisonnalité.
Il faut génerer les variables explicatives pour ajuster les variables du modèle au sens du MCO. 

```{r}
t <- 1:length(ts_serie_semaine)
x <- outer(t,1:6)*(pi/6)
df <- data.frame(ts_serie_semaine,t,cos(x),sin(x[,-6]))
# 
# 
# x <- matrix(1,nrow=nrow(ts_serie_semaine),ncol=9) # car 9 régresseurs au total
# t <- 1:nrow(ts_serie_semaine)
# x[,2] <- t
# x[,3] <- t**2
# x[,5] <- cos((2*pi*t)/7)
# x[,6] <- cos((4*pi*t**2)/7)
# x[,7] <- sin((2*pi*t)/7)
# x[,8] <- sin((4*pi*t**2)/7)

ts_serie1_lm <- lm(data=df,ts_serie1~.)
```


## Etude des résidus 

La fonction Box.test examine l’hypothèse nulle de nullité des H premières auto-covariance.
Par défaut H est fixé à 1, et seule la nullité de l’auto-covariance d’ordre 1 est testée.

Pour tester si la série peut-être apparentée à un bruit blanc, nous fixerons un H de l’ordre de 7.

```{r}
Box.test(ts_serie_semaine,lag=7)
```
Puisque la p-value est inférieure à 5%, on rejette l'hypothèse de non-autocorrélation. Cela implique que la série temporelle présente une autocorrélation significative et que les valeurs successives de la série temporelle sont dépendantes les unes des autres.

La fonction ks.test() est une fonction de test de Kolmogorov-Smirnov dans R, qui permet de comparer une distribution empirique à une distribution théorique normale.

```{r}
ks.test(ts_serie_semaine, "pnorm", mean(ts_serie_semaine), sd(ts_serie_semaine))
```
Les données ne suivent pas une distribution théorique car la p-value est inférieure à 5%. On rejette donc l'hypothèse nulle.

# Comparaison des modèles 

```{r}
c(AIC(model_arima),AIC(ts_serie1_lm))
c(BIC(model_arima),BIC(ts_serie1_lm))
```

Nous aurons tendance à privilégier le modèle ARIMA puisque les critères de l'AIC et le BIC sont minimisés. 

Il aurait été intéressant d'effectuer un test anova pour comparer les modèles. Cependant, nous ne pouvons pas utiliser la méthode anova() sur un objet de classe Arima. 

# Prévisions 

Suite à notre analyse, nous avons utilisé la fonction forecast pour émettre des prévisions sur notre série pour les 7 prochains jours.

```{r}
forecast_cases <- forecast::forecast(model_arima, h = 7)
plot(forecast_cases)
```
Les prédictions : 

```{r}
predict(model_arima)
```

Nos prévisions ne sont pas très précises puisque notre série n'est pas stationnaire.  




# Conclusion
